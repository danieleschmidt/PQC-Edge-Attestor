/**
 * @file autoScaling.js
 * @brief Intelligent auto-scaling middleware for Generation 3
 */

const { performance } = require('perf_hooks');
const winston = require('winston');
const os = require('os');

// Auto-scaling logger
const scalingLogger = winston.createLogger({
    level: 'info',
    format: winston.format.combine(
        winston.format.timestamp(),
        winston.format.json()
    ),
    transports: [
        new winston.transports.File({ filename: 'logs/scaling.log' }),
        new winston.transports.Console()
    ]
});

class AutoScalingManager {
    constructor(options = {}) {
        this.options = {
            cpuThresholdHigh: options.cpuThresholdHigh || 80,
            cpuThresholdLow: options.cpuThresholdLow || 30,
            memoryThresholdHigh: options.memoryThresholdHigh || 85,
            memoryThresholdLow: options.memoryThresholdLow || 40,
            responseTimeThresholdHigh: options.responseTimeThresholdHigh || 1000,
            responseTimeThresholdLow: options.responseTimeThresholdLow || 200,
            scaleUpCooldown: options.scaleUpCooldown || 300000, // 5 minutes
            scaleDownCooldown: options.scaleDownCooldown || 600000, // 10 minutes
            minInstances: options.minInstances || 1,
            maxInstances: options.maxInstances || 10,
            evaluationPeriod: options.evaluationPeriod || 60000, // 1 minute
            dataPoints: options.dataPoints || 5,
            ...options
        };\n        \n        this.metrics = {\n            cpuUsage: [],\n            memoryUsage: [],\n            responseTime: [],\n            requestRate: [],\n            errorRate: []\n        };\n        \n        this.scalingHistory = [];\n        this.lastScaleAction = null;\n        this.currentInstances = 1;\n        this.targetInstances = 1;\n        \n        // Worker pool for request processing\n        this.workers = new Map();\n        this.workQueue = [];\n        this.processedRequests = 0;\n        this.failedRequests = 0;\n        \n        // Start monitoring\n        this.startMonitoring();\n        this.initializeWorkerPool();\n    }\n    \n    startMonitoring() {\n        setInterval(() => {\n            this.collectMetrics();\n            this.evaluateScaling();\n        }, this.options.evaluationPeriod);\n        \n        scalingLogger.info('Auto-scaling monitoring started', {\n            evaluationPeriod: this.options.evaluationPeriod,\n            thresholds: {\n                cpu: `${this.options.cpuThresholdLow}-${this.options.cpuThresholdHigh}%`,\n                memory: `${this.options.memoryThresholdLow}-${this.options.memoryThresholdHigh}%`\n            }\n        });\n    }\n    \n    collectMetrics() {\n        const memUsage = process.memoryUsage();\n        const cpuUsage = this.getCPUUsage();\n        \n        // Collect current metrics\n        const currentMetrics = {\n            cpu: cpuUsage,\n            memory: (memUsage.heapUsed / memUsage.heapTotal) * 100,\n            responseTime: this.getAverageResponseTime(),\n            requestRate: this.getRequestRate(),\n            errorRate: this.getErrorRate(),\n            timestamp: Date.now()\n        };\n        \n        // Store metrics with sliding window\n        Object.keys(this.metrics).forEach(key => {\n            if (key in currentMetrics) {\n                this.metrics[key].push(currentMetrics[key]);\n                \n                // Keep only last N data points\n                if (this.metrics[key].length > this.options.dataPoints) {\n                    this.metrics[key].shift();\n                }\n            }\n        });\n        \n        scalingLogger.debug('Metrics collected', currentMetrics);\n    }\n    \n    getCPUUsage() {\n        const cpus = os.cpus();\n        let totalIdle = 0;\n        let totalTick = 0;\n        \n        cpus.forEach(cpu => {\n            for (const type in cpu.times) {\n                totalTick += cpu.times[type];\n            }\n            totalIdle += cpu.times.idle;\n        });\n        \n        return Math.round((1 - totalIdle / totalTick) * 100);\n    }\n    \n    getAverageResponseTime() {\n        // Mock response time calculation\n        return Math.random() * 500 + 100; // 100-600ms\n    }\n    \n    getRequestRate() {\n        // Mock request rate calculation\n        return Math.random() * 100; // 0-100 requests/minute\n    }\n    \n    getErrorRate() {\n        // Mock error rate calculation\n        return Math.random() * 5; // 0-5% error rate\n    }\n    \n    evaluateScaling() {\n        if (this.metrics.cpuUsage.length < this.options.dataPoints) {\n            scalingLogger.debug('Insufficient data points for scaling evaluation');\n            return;\n        }\n        \n        const avgCpu = this.getAverage(this.metrics.cpuUsage);\n        const avgMemory = this.getAverage(this.metrics.memoryUsage);\n        const avgResponseTime = this.getAverage(this.metrics.responseTime);\n        \n        const scaleUpConditions = [\n            avgCpu > this.options.cpuThresholdHigh,\n            avgMemory > this.options.memoryThresholdHigh,\n            avgResponseTime > this.options.responseTimeThresholdHigh\n        ];\n        \n        const scaleDownConditions = [\n            avgCpu < this.options.cpuThresholdLow,\n            avgMemory < this.options.memoryThresholdLow,\n            avgResponseTime < this.options.responseTimeThresholdLow\n        ];\n        \n        const shouldScaleUp = scaleUpConditions.some(condition => condition);\n        const shouldScaleDown = scaleDownConditions.every(condition => condition);\n        \n        scalingLogger.debug('Scaling evaluation', {\n            avgCpu,\n            avgMemory,\n            avgResponseTime,\n            shouldScaleUp,\n            shouldScaleDown,\n            currentInstances: this.currentInstances\n        });\n        \n        if (shouldScaleUp && this.canScaleUp()) {\n            this.scaleUp({\n                reason: 'High resource utilization',\n                metrics: { avgCpu, avgMemory, avgResponseTime }\n            });\n        } else if (shouldScaleDown && this.canScaleDown()) {\n            this.scaleDown({\n                reason: 'Low resource utilization',\n                metrics: { avgCpu, avgMemory, avgResponseTime }\n            });\n        }\n    }\n    \n    canScaleUp() {\n        const now = Date.now();\n        const lastScaleUp = this.getLastScaleAction('scale_up');\n        \n        return (\n            this.currentInstances < this.options.maxInstances &&\n            (!lastScaleUp || (now - lastScaleUp.timestamp) > this.options.scaleUpCooldown)\n        );\n    }\n    \n    canScaleDown() {\n        const now = Date.now();\n        const lastScaleDown = this.getLastScaleAction('scale_down');\n        \n        return (\n            this.currentInstances > this.options.minInstances &&\n            (!lastScaleDown || (now - lastScaleDown.timestamp) > this.options.scaleDownCooldown)\n        );\n    }\n    \n    async scaleUp(context) {\n        const newInstances = Math.min(\n            this.currentInstances + 1,\n            this.options.maxInstances\n        );\n        \n        await this.executeScaling('scale_up', newInstances, context);\n    }\n    \n    async scaleDown(context) {\n        const newInstances = Math.max(\n            this.currentInstances - 1,\n            this.options.minInstances\n        );\n        \n        await this.executeScaling('scale_down', newInstances, context);\n    }\n    \n    async executeScaling(action, targetInstances, context) {\n        const scalingEvent = {\n            id: this.generateId(),\n            action,\n            fromInstances: this.currentInstances,\n            toInstances: targetInstances,\n            reason: context.reason,\n            metrics: context.metrics,\n            timestamp: Date.now(),\n            status: 'in_progress'\n        };\n        \n        this.scalingHistory.push(scalingEvent);\n        this.lastScaleAction = scalingEvent;\n        \n        scalingLogger.info('Scaling action initiated', scalingEvent);\n        \n        try {\n            if (action === 'scale_up') {\n                await this.addWorkers(targetInstances - this.currentInstances);\n            } else {\n                await this.removeWorkers(this.currentInstances - targetInstances);\n            }\n            \n            this.currentInstances = targetInstances;\n            this.targetInstances = targetInstances;\n            \n            scalingEvent.status = 'completed';\n            scalingEvent.completedAt = Date.now();\n            \n            scalingLogger.info('Scaling action completed', {\n                id: scalingEvent.id,\n                instances: this.currentInstances\n            });\n            \n        } catch (error) {\n            scalingEvent.status = 'failed';\n            scalingEvent.error = error.message;\n            scalingEvent.completedAt = Date.now();\n            \n            scalingLogger.error('Scaling action failed', {\n                id: scalingEvent.id,\n                error: error.message\n            });\n        }\n    }\n    \n    initializeWorkerPool() {\n        for (let i = 0; i < this.currentInstances; i++) {\n            this.createWorker();\n        }\n    }\n    \n    createWorker() {\n        const workerId = this.generateId();\n        const worker = {\n            id: workerId,\n            status: 'idle',\n            requestsProcessed: 0,\n            lastUsed: Date.now(),\n            created: Date.now()\n        };\n        \n        this.workers.set(workerId, worker);\n        \n        scalingLogger.debug('Worker created', { workerId });\n        \n        return worker;\n    }\n    \n    async addWorkers(count) {\n        for (let i = 0; i < count; i++) {\n            this.createWorker();\n            // Simulate worker startup time\n            await new Promise(resolve => setTimeout(resolve, 100));\n        }\n        \n        scalingLogger.info('Workers added', { count, totalWorkers: this.workers.size });\n    }\n    \n    async removeWorkers(count) {\n        const workerIds = Array.from(this.workers.keys()).slice(0, count);\n        \n        for (const workerId of workerIds) {\n            const worker = this.workers.get(workerId);\n            \n            // Wait for worker to finish current request\n            if (worker.status === 'busy') {\n                await this.waitForWorkerIdle(workerId);\n            }\n            \n            this.workers.delete(workerId);\n        }\n        \n        scalingLogger.info('Workers removed', { count, totalWorkers: this.workers.size });\n    }\n    \n    async waitForWorkerIdle(workerId, timeout = 30000) {\n        const startTime = Date.now();\n        \n        while (Date.now() - startTime < timeout) {\n            const worker = this.workers.get(workerId);\n            \n            if (!worker || worker.status === 'idle') {\n                return;\n            }\n            \n            await new Promise(resolve => setTimeout(resolve, 100));\n        }\n        \n        scalingLogger.warn('Worker idle timeout', { workerId });\n    }\n    \n    // Express middleware for auto-scaling\n    middleware() {\n        return async (req, res, next) => {\n            const startTime = performance.now();\n            \n            // Get available worker\n            const worker = await this.getAvailableWorker();\n            \n            if (!worker) {\n                // No workers available, queue the request\n                this.workQueue.push({ req, res, next, startTime });\n                return;\n            }\n            \n            // Assign request to worker\n            worker.status = 'busy';\n            worker.lastUsed = Date.now();\n            \n            // Process request\n            const originalSend = res.send;\n            res.send = function(data) {\n                const duration = performance.now() - startTime;\n                \n                // Update worker stats\n                worker.status = 'idle';\n                worker.requestsProcessed++;\n                \n                // Update global stats\n                if (res.statusCode >= 400) {\n                    this.failedRequests++;\n                } else {\n                    this.processedRequests++;\n                }\n                \n                // Process queued requests\n                this.processQueue();\n                \n                return originalSend.call(this, data);\n            }.bind(this);\n            \n            next();\n        };\n    }\n    \n    async getAvailableWorker() {\n        for (const [workerId, worker] of this.workers) {\n            if (worker.status === 'idle') {\n                return worker;\n            }\n        }\n        \n        return null;\n    }\n    \n    processQueue() {\n        while (this.workQueue.length > 0) {\n            const worker = this.getAvailableWorker();\n            \n            if (!worker) {\n                break;\n            }\n            \n            const queuedRequest = this.workQueue.shift();\n            \n            // Assign queued request to worker\n            worker.status = 'busy';\n            worker.lastUsed = Date.now();\n            \n            // Continue processing the queued request\n            queuedRequest.next();\n        }\n    }\n    \n    getAverage(values) {\n        if (values.length === 0) return 0;\n        return values.reduce((sum, val) => sum + val, 0) / values.length;\n    }\n    \n    getLastScaleAction(actionType) {\n        return this.scalingHistory\n            .filter(event => event.action === actionType)\n            .sort((a, b) => b.timestamp - a.timestamp)[0];\n    }\n    \n    generateId() {\n        return Math.random().toString(36).substr(2, 9);\n    }\n    \n    getScalingMetrics() {\n        const now = Date.now();\n        const last24Hours = now - (24 * 60 * 60 * 1000);\n        \n        const recentEvents = this.scalingHistory.filter(event => \n            event.timestamp > last24Hours\n        );\n        \n        return {\n            current: {\n                instances: this.currentInstances,\n                targetInstances: this.targetInstances,\n                workers: this.workers.size,\n                queuedRequests: this.workQueue.length\n            },\n            limits: {\n                min: this.options.minInstances,\n                max: this.options.maxInstances\n            },\n            metrics: {\n                processedRequests: this.processedRequests,\n                failedRequests: this.failedRequests,\n                successRate: this.processedRequests > 0 ?\n                    Math.round((this.processedRequests / (this.processedRequests + this.failedRequests)) * 100) :\n                    0\n            },\n            recentActivity: {\n                scalingEvents: recentEvents.length,\n                scaleUps: recentEvents.filter(e => e.action === 'scale_up').length,\n                scaleDowns: recentEvents.filter(e => e.action === 'scale_down').length\n            },\n            lastScaleAction: this.lastScaleAction ? {\n                ...this.lastScaleAction,\n                timeAgo: now - this.lastScaleAction.timestamp\n            } : null,\n            workers: Array.from(this.workers.values()).map(worker => ({\n                id: worker.id,\n                status: worker.status,\n                requestsProcessed: worker.requestsProcessed,\n                uptime: now - worker.created\n            }))\n        };\n    }\n    \n    // Manual scaling controls\n    async manualScale(targetInstances, reason = 'Manual scaling') {\n        if (targetInstances < this.options.minInstances || \n            targetInstances > this.options.maxInstances) {\n            throw new Error(`Target instances must be between ${this.options.minInstances} and ${this.options.maxInstances}`);\n        }\n        \n        const action = targetInstances > this.currentInstances ? 'scale_up' : 'scale_down';\n        \n        await this.executeScaling(action, targetInstances, {\n            reason,\n            metrics: {}\n        });\n        \n        return this.getScalingMetrics();\n    }\n    \n    // Predictive scaling based on historical patterns\n    enablePredictiveScaling() {\n        setInterval(() => {\n            this.predictiveScaling();\n        }, 300000); // Every 5 minutes\n        \n        scalingLogger.info('Predictive scaling enabled');\n    }\n    \n    predictiveScaling() {\n        // Simple predictive logic based on historical patterns\n        const now = new Date();\n        const hour = now.getHours();\n        \n        // Peak hours prediction\n        const isPeakHour = (hour >= 9 && hour <= 17) || (hour >= 19 && hour <= 22);\n        \n        if (isPeakHour && this.currentInstances === this.options.minInstances) {\n            scalingLogger.info('Predictive scaling: preparing for peak hours');\n            \n            this.scaleUp({\n                reason: 'Predictive scaling - peak hours',\n                metrics: { predictive: true, hour }\n            });\n        }\n    }\n}\n\n// Global auto-scaling manager\nconst autoScalingManager = new AutoScalingManager();\n\nmodule.exports = {\n    AutoScalingManager,\n    autoScalingManager\n};